---
title: çº¿æ€§ä»£æ•°åŸºç¡€ä¸€
author: èŒƒè€€æ–‡
top: false
cover: false
toc: true
mathjax: true
date: 2020-02-25 09:46:58
img: /medias/banner/linear_math.jpg
coverImg: 
password:
summary:
	æ€»ç»“äº†çº¿æ€§ä»£æ•°å¸¸ç”¨çš„çŸ¥è¯†ç‚¹ã€‚
tags:
	- çº¿æ€§ä»£æ•°
categories:
	- çº¿æ€§ä»£æ•°
typora-root-url: ..
---

# basic conception

## Zero Matrix

matrix with all zero entries, denoted by $O$ (any size) or $O_{m \times n}$

For example, a 2-by-3 zero matrix can be denoted
$$
O_{2 \times 3} = \left[
\begin{matrix}
  0 & 0 & 0\\
  0 & 0 & 0 \\
\end{matrix}
\right]
$$

## Identity Matrix

 must be square,Sometimes $ I_n $  is simply written as $I$ (any size). 
$$
I_3 = \left[
\begin{matrix}
  1 & 0 & 0\\
  0 & 1 & 0 \\
  0 & 0 & 1
\end{matrix}
\right]
$$
## $(0,1)$ matrices

all the entities of A are zeros and ones called (0,1)matrices.
$$
I_3 = \left[
\begin{matrix}
  0&1 & 1 & 0\\
  1 & 0 & 0&1 \\
  1&0&0&0\\
  0 & 1 & 0&0
\end{matrix}
\right]
$$
 <font color=red >get symmetirc matrix:$A^TA$</font>   

åœ¨ç”Ÿæ´»ä¸­ï¼Œæˆ‘ä»¬ä¼šç»å¸¸ç”¨åˆ°(0,1)çŸ©é˜µ(æ¶‰åŠå›¾)

## Standard Vectors

we can write any vector $\left[
\begin{matrix}
  {a}\\
  {b}\\
\end{matrix}
\right]$in $R^2$ as a linear combination of the two vectors $\left[
\begin{matrix}
  {1}\\
  {0}\\
\end{matrix}
\right]$and$\left[
\begin{matrix}
  {0}\\
  {1}\\
\end{matrix}
\right]$

as follows:
$$
\left[
\begin{matrix}
  {a}\\
  {b}\\
\end{matrix}
\right]=a\left[
\begin{matrix}
  {1}\\
  {0}\\
\end{matrix}
\right]+b\left[
\begin{matrix}
  {0}\\
  {1}\\
\end{matrix}
\right]
$$
the vectors $\left[
\begin{matrix}
  {1}\\
  {0}\\
\end{matrix}
\right]$ and$\left[
\begin{matrix}
  {0}\\
  {1}\\
\end{matrix}
\right]$ are called the standard vectors of $R^2$ .

In general, we define the standard vectors of $R^n$ by
$$
e_1 =\left[
\begin{matrix}
  {1}\\
  {0}\\
  {\vdots}\\
  {0}
\end{matrix}
\right]
\qquad e_2 =\left[
\begin{matrix}
  {0}\\
  {1}\\
  {\vdots}\\
  {0}
\end{matrix}
\right]\qquad \cdots \qquad e_n =\left[
\begin{matrix}
  {0}\\
  {0}\\
  {\vdots}\\
  {1}
\end{matrix}\right]
$$

## Linear Combination

+ Given a vector set $\{u_1,u_2,\cdots,u_k\}$
+ The linear combination of the vectors in the set: 
  +  $ğ‘£ = ğ‘1ğ‘¢1 + ğ‘2ğ‘¢2 + â‹¯ + ğ‘ğ‘˜ğ‘¢ğ‘˜$
  + $ ğ‘1, ğ‘2, â‹¯ , ğ‘ğ‘˜ $ are scalars (Coefficients of linear combination)
+ the set of coefficients that express one vector as a linear combination of the others need not be unique
+ ä»¥çº¿æ€§æ–¹ç¨‹ç»„çš„æ€æƒ³ç†è§£çº¿æ€§ç»„åˆ

$$
\begin{equation}  
\left\{
  \begin{array}{c}
  a_{11}x_1+a_{12}x_{2}+...+a_{1n}x_{n} = b_{1} \\
  a_{21}x_1+a_{22}x_{2}+...+a_{2n}x_{n} = b_{2} \\
          \vdots\\
  a_{m1}x_1+a_{m2}x_{n}+...+a_{mn}x_{n} = b_{n}
  \end{array}
\right.
\end{equation}
$$

$$
A =\left[
\begin{matrix}
  \mathbf{a_1}&\mathbf{a_2}&\mathbf{\cdots},\mathbf{a_n}
\end{matrix}
\right]
$$

$$
x =\left[
\begin{matrix}
  {a_1}\\
  {a_2}\\
  {\vdots}\\
  {a_n}
\end{matrix}
\right]
\qquad coeffients
$$

$$
\color{blue}{A}\color{red}{x} \color{black}=\color{red}{x_1}\color{blue}{a_1}\color{black}+\color{red}{x_2}\color{blue}{a_2}\color{black}+\color{red}{x_3}\color{blue}{a_3}\color{black}+\cdots\color{black}+\color{red}{x_n}\color{blue}{a_n}
$$

$A{\color{red}{x}}=b$ï¼šbæ˜¯å‘é‡é›†Açš„çº¿æ€§ç»„åˆï¼Œxæ˜¯ç³»æ•°é›†ï¼Œè‹¥æ–¹ç¨‹ç»„æœ‰æ— æ•°è§£ï¼Œåˆ™xä¸å”¯ä¸€ã€‚

## Span

+ A vector set $S=\{u_1,u_2,{\cdots},u_k\}$ 
+ Span of ğ‘† is the vector set of all linear combinations of $ğ‘¢_1$, $ğ‘¢_2$, â‹¯ , $ğ‘¢_k$ 
  + Denoted by ğ‘†ğ‘ğ‘ğ‘› $\{ğ‘¢_1,ğ‘¢_2, â‹¯ ,ğ‘¢_ğ‘˜ \}$ or Sğ‘ğ‘ğ‘›  S
  
  + ğ‘†ğ‘ğ‘ğ‘› ğ‘† = $\{c_1u_1+c_2u_2+\cdots+c_ku_kfor\quad all |c_1,c_2,\cdots,c_k\}$ 
  
  + Vector set $ğ‘‰$ = ğ‘†ğ‘ğ‘ğ‘› $S$
  
     -  ğ‘† is a generating set for ğ‘‰â€ or â€œğ‘† generates Vâ€ 
  -  A vector set generated by another vector set is called <font color=red >Space </font>
     
  

<img src="/images/linear_algebra/span.png" alt="span" style="zoom:50%;" />

+ m independent vectors can span $R^m$ --->More than m vectors in $R^m $ must be dependent
+ m independent vectors can span $R^m$ --->More than m vectors in $R^m$  must be dependent

## Dependent and Independent

A set of n vectors $\{ğ’‚_1,ğ’‚_2,\cdots,ğ’‚_ğ‘›\}$ is linear dependent 

 +   If there exist scalars ğ‘¥1, ğ‘¥2, â‹¯ , ğ‘¥ğ‘›, <font color=red>not all zero</font>,

 +  such that 

    $$ğ‘¥_1ğ’‚_1 + ğ‘¥_2ğ’‚_2 +\cdots+ ğ‘¥_ğ‘›ğ’‚_ğ‘›=0$$

 +   A set of n vectors $\{ğ’‚_1, ğ’‚_2,\cdots, ğ’‚_ğ‘›\}$ is linear independent 

     $$ğ‘¥_1ğ’‚_1 + ğ‘¥_2ğ’‚_2 +\cdots+ ğ‘¥_ğ‘›ğ’‚_ğ‘›=0$$

    Only if $ğ‘¥1 = ğ‘¥2 = â‹¯ = ğ‘¥ğ‘› = 0$ 

<font color=blue >Any set contains zero vector would be linear dependent</font> 

## Reduced row echelon form

+ Row Echelon Form

  +  Each nonzero row lies above every zero rowï¼ˆall its entries are 0ï¼‰
  + The leading entries are in echelon form(é˜¶æ¢¯å‹)

+ reduced row echelon form

  + The matrix is in row echelon form

  + The columns containing the leading entries are standard vectors

    <img src="/images/linear_algebra/ref.png" alt="ref" style="zoom:50%;" />

+ RREFåœ¨æ±‚è§£çº¿æ€§æ–¹ç¨‹ç»„ä¸­çš„åº”ç”¨
  + ä¸€ä¸ªçº¿æ€§æ–¹ç¨‹ç»„å¯ä»¥ç”¨å¢å¹¿çŸ©é˜µæ¥è¡¨ç¤ºï¼Œå¯¹å¢å¹¿çŸ©é˜µè¿›è¡Œä»»ä½•çš„åŸºæœ¬è¡Œæ“ä½œï¼Œè§£é›†ä¸å˜
  + å½¢å¦‚ç®€åŒ–é˜¶æ¢¯å½¢çŸ©é˜µå¯ä»¥å¾ˆå®¹å™¨æ±‚å‡ºè§£é›†ã€‚
    - å½“rrefçŸ©é˜µåŒ…å«é™¤æœ€åä¸€ä¸ªå…ƒç´ éé›¶çš„ä¸€è¡Œï¼šæ— è§£
    - å½“rrefçŸ©é˜µçš„ç³»æ•°çŸ©é˜µä¸ºå•ä½çŸ©é˜µï¼šå”¯ä¸€è§£
    - å…¶ä»–æƒ…å†µï¼šæ— ç©·è§£
      - basic variables
      - free variables

+ RREFçš„æ€§è´¨
  + COLUMNS: If ğ’‚ğ’‹ is a linear combination of other columns of A( with the same coefficients)<---> ğ’“ğ’‹ is a linear combination of the corresponding columns of R with the same coefficients
  + ROWS: span$\{row_1,row_2,\cdots,row_n\}$ <--->span$\{row^1,row^2,\cdots,row^n\}$ 
  + The pivot columns are linear independent
  + The non-pivot columns are the linear combination of the previous pivot columns.

## Having Solution or Not of system of linear equations

+ Is ğ‘ a linear combination of columns of ğ´?
+ Is ğ‘ in the span of the columns of ğ´?
+ is consistent?
+ is b in Col A?

## How many solutions

+ Columns of A are dependent <--->If Ax=b have solution, it will have Infinite solutions

+ Columns of A are independent <--->if Ax=b have solution ,it will have only one solution


## Rank and Nullity 

+ definition_1
  + The rank of a matrix is defined as the maximum number of <font color=blue>linearly independent columns</font>> in the matrix. 
  + Nullity = <font color=red>Number of columns</font> - rank
+ definition_2
  + Rank=Number of Pivot Column
+ definition_3
  + Rank=Number of Non-zero rows
+ definition_4
  + Rank=Number of Basic Variables
+ definition_5
  + Rank= Dim (Col A): dimension of column space
  + Dimension of the range of A
+ definition_6
  + Rank=Dim(Row A): dimension of row space
+ properties
  + Rank $A$=n Nullity=0  <--->   Columns of $A$ are independent
  + Given a $m\times n$ matrix $A$:$
    + Matrix $A$ is full rank if Rank $A$ = min(m,n)
    + In $R^m$, you cannot find more than m vectors that are independent
    + rank $A$ = rank $A^T$  

## Inverse of a Matrix

+ definition:

  $A$ is called invertible if there is a matrix B such that $ğ´ğµ = ğ¼$  and $ğµğ´ = I$ 

+ elementary matrices

  Every elementary row operation can be performed by matrix multiplication

  +  Interchange
    $$
    \color{blue}{\left[
    \begin{matrix}
      0 & 1 \\
      1 & 0 \\
    \end{matrix}
    \right]}
    \color{black}{\left[
    \begin{matrix}
      a & b \\
      c & d \\
    \end{matrix}
    \right]
    =
    \left[
    \begin{matrix}
      c & d \\
      a & b \\
    \end{matrix}
    \right]}
    $$

  + Scaling
    $$
    \color{blue}{\left[
    \begin{matrix}
      1 & 0 \\
      0 & k \\
    \end{matrix}
    \right]}
    \color{black}{\left[
    \begin{matrix}
      a & b \\
      c & d \\
    \end{matrix}
    \right]
    =
    \left[
    \begin{matrix}
      a & b \\
      kc & kd \\
    \end{matrix}
    \right]}
    $$

  +  Adding k times row i to row j
    $$
    \color{blue}{\left[
    \begin{matrix}
      1 & 0 \\
      k & 1 \\
    \end{matrix}
    \right]}
    \color{black}{\left[
    \begin{matrix}
      a & b \\
      c & d \\
    \end{matrix}
    \right]
    =
    \left[
    \begin{matrix}
      a & b \\
      ka+c & kb+d \\
    \end{matrix}
    \right]}
    $$

+ Inverse of Elementary Matrix

  Reverse elementary row operation

$$
E_1=\left[
\begin{matrix}
  1 & 0 \\
  k & 1 \\
\end{matrix}
\right]
E_1^{-1}=\left[
\begin{matrix}
  1 & 0 \\
  -k & 1 \\
\end{matrix}
\right]
$$

+ RREF & Elementary Matrix

  + Let A be an $m \times n$ matrix with reduced row echelon form R

  $$
  ğ¸_{ğ‘˜}\cdots ğ¸_{2}ğ¸_1ğ´ = R
  $$

  + There exists an invertible m x m matrix P such that PA=R
    $$
    ğ‘ƒ^{âˆ’1} = ğ¸_1^{âˆ’1}ğ¸_2^{âˆ’1}\cdots ğ¸_ğ‘˜^{âˆ’1}
    $$

+ åˆ¤æ–­ä¸€ä¸ªçŸ©é˜µæ˜¯å¦å¯é€†
  
  Let A be an n x n matrix. A is invertible if and only if
  
  + The columns of $A$ span $R_n$
  + For every $b$  in $R_n$ , the system $Ax=b$  is consistent
  + The rank of $A$  is $n$ 
  + The columns of $A$  are linear independent
  + The only solution to $Ax=0$  is the zero vector
  + The nullity of $A$  is zero
  + The reduced row echelon form of $A$ is $I_n$
  + $A$ is a product of elementary matrices
  + There exists an $n \times n$ matrix $B$ such that $BA = I_n$
  + There exists an $n \times n $ matrix $C$ such that $AC = I_n$ 

# Subspace

+ definition
  +  The zero vector $0$  belongs to $V$ 
  +  If u and $w$  belong to $V$ , then $u+w$  belongs to $V$ (Closed under (vector) addition)
  +  If u belongs to $V$ , and $c$  is a scalar, then $cu$  belongs to $V$ (Closed under scalar multiplication)

+ special

  zero subspace: the set $w$ consisting of only the zero vector in $R_n$ is a subspace of $R_n$ called the **zero subspace** . A subspace of $R_n$ other than ${0}$ is called a **nonzero subspace**.

+ properties
  + The span of a vector set is a subspace
  + The **null space** of a matrix A is the solution set of Ax=0. It is denoted as Null A.
  + The **column space** of a matrix A is the span of its columns. it is denoted by Col A.
  + **Row space** of a matrix A is the span of its rows. It is denoted as Row A. 

## Basis

+ definition

  + Let $V$ be a nonzero subspace of $R_n$. A basis $B$ for $V$ is a linearly independent generation set of $V$ 
  + $\{e_1,e_2,\cdots,e_n\}$ is a basis for $R_n$ 
    +  $\{e_1,e_2,\cdots,e_n \}$ is independent
    +  $\{e_1 , e_2 , \cdots, e_n \}$ generates $R$

  + The pivot columns of a matrix form a basis for its columns space.

+ properties

  + $S$ is contained in Span $S$
  + If a finite set Sâ€™ is contained in Span $S$, then Span $Sâ€™$ is also contained in Span $S$
  + For any vector $z$, Span $S$ = Span $Sâˆª{z}$ if and only if z belongs to the Span $ S$

+ Theorem 

  +  A basis is the <font color=blue>smallest generation set</font>.

  + A basis is the <font color=red>largest independent vector set</font> in the subspace.

  +  Any two bases for a subspace <font color=green>contain the same number of vectors</font>.

    + The number of vectors in a basis for a nonzero subspace V is called <font color=red>dimension</font> of $V $(<font color=red>dim</font> $V$).

+ Subspaces associated with a Matrix

  <img src="/images/linear_algebra/basis.png" alt="basis" style="zoom:50%;" />

## linear transformations 

+ **function**

  let $S_1$ and $S_2$ be subsets of $R_n$ and $R_m$, respectively. A **function** $f$ from $S_1$ and $S_2$. written f :$S_1->S_2$. is a rule that assigns to each vector $v$ in $S_1$ A unique vector $f(v)$ in $S_2$ .The vector $f(v)$ is called the image of $v$ (under $f$). The set $S_1$ is called the **domain** of a function $f$. and the set $S_2$ is called the **codomain** of $f$. The $range$ of $f$ is defined to be the set of images $f(v)$ for all $v$ in $S_1$.
  
  <img src="/images/linear_algebra/T_a.jpg" alt="function" style="zoom:50%;" />

+ **matrix transformation**

  <font color=blue>let A be an $m \times n$ matrix. The function $T_A:R^n ->R^m$ defined by $T_A(X)=Ax$ for all $x$ in $R^n$ is called the **matrix transformation** induced by A</font>.

  + rotations 

  $$
  A_0 = \left[
  \begin{matrix}
    cos \theta & -sin \theta\\
    sin \theta & cos \theta 
  \end{matrix}
  \right]
  $$
  + Expansion
    $$
    T_A = \left[
    \begin{matrix}
      2 & 0 \\
      0 & 2  \\
    \end{matrix}
    \right]
    $$

  + compression
    $$
    T_A = \left[
    \begin{matrix}
      0.5 & 0 \\
      0 & 0.5  \\
    \end{matrix}
    \right]
    $$

  + projections

    å°†ä¸€ä¸ªå‘é‡æ˜ å°„åˆ°$xy-plane$

  $$
  T_A = \left[
  \begin{matrix}
    1 & 0 & 0\\
    0 & 1 & 0 \\
    0 & 0 & 0
  \end{matrix}
  \right]
  $$

  + shear

$$
T_A = \left[
\begin{matrix}
  1 & k \\
  0 & 1  \\
\end{matrix}
\right]
$$

â€‹		  



+ **linear transformation**

  <font color=RED>A function T from $R^n$ to $R^m$. written $T: R^n ->R^m$. is called linear transformation if for all vectors $u$ and $v$ in $R^n$ and all scalar c </FONT>.

  + $T(u+v) = T(u)+T(v)$
  + $T(cu) = cT(u)$

  special linear transformation

  + matrix transformation
  + identity transformation
  + zero transformation

+ standard matrix

  let $T: R^n ->R^m$ be a linear transformation. we call the $m \times n$ matrix 

  $A=[T(e_1) T(e_2)\cdots T(e_n)]$ 

  $T(v)=Av$ 

  

## coordinate system

+ definition

  + Let vector set $B=\{ğ‘¢_1,ğ‘¢_2, â‹¯,ğ‘¢_ğ‘›$ be a **<font color=red>basis</font>** for a subspace $R^n$ ,$B$ is a coordinate system

  + For any $v$ in $R^n$ , there are unique scalars $ğ‘_1,ğ‘_2,â‹¯,ğ‘_ğ‘›$ such that $ğ‘£ = ğ‘_1ğ‘¢_1 + ğ‘_2ğ‘¢_2 + â‹¯ + ğ‘_ğ‘›ğ‘¢_n$ , <font color=red>**$B$ -coordinate vector**</font> of $ v$:
    $$
    [v]_B =\left[
    \begin{matrix}
      {c_1}\\
      {c_2}\\
      {\vdots}\\
      {c_n}
    \end{matrix}
    \right]
    \in R^n
    $$

<img src="/images/linear_algebra/coordinate.png" alt="coordinate system" style="zoom:50%;" />

## Linear Function in Coordinate System

<img src="/images/linear_algebra/linear_opertor.png" alt="linear_opertor" style="zoom:50%;" />

<img src="/images/linear_algebra/matirx_representation.png" alt="matirx_representation" style="zoom:50%;" />

# EIGENVALUES, EIGENVECTORS

## Eigenvalues and Eigenvectors

+ definition

  If $ğ´ğ‘£ = ğœ†ğ‘£$ ($ğ‘£$ is a vector, $ğœ†$ is a scalar)

  + $ğ‘£$ is an eigenvector of $A$(excluding zero vector)
  + $ğœ†$ is an eigenvalue of $A$ that corresponds to $v$

+ properties
  + An eigenvector of A corresponds to a unique eigenvalue
  + An eigenvalue of A has infinitely many eigenvectors

+ eigenspace

  $ğ‘ğ‘¢ğ‘™ğ‘™(A âˆ’ \lambda I_n )$

+ how to look for the eigenvalue

  $det(A-tI_n)=0$ characteristic equation

  $det(A-tI_n)$ characteristic polynomial

  **Eigenvalues are the roots of characteristic polynomial or solutions of characteristic equation**

## Characteristic Polynomial

+ definition

  Characteristic polynomial of A is

$$
det(A-tI_n)=(t-\lambda_1)^{m_1}(t-\lambda_2)^{m_2}\cdots(t-\lambda_k)^{m_k}(\cdots)
$$

â€‹	$m_k \text{ is call the multiplicity of }\lambda$  

An n x n matrix A have less than or equal to n eigenvalues

+ properties

  let $\lambda$ be an eigenvalue of a matrix A .the dimension of the eigenspace of $A$ corresponding to $\lambda$  is less than or equal to the multiplicity of $\lambda$.

## The eigenvalues of similar matrices

Similar matrices have the same characteristic polynomials

The same Eigenvalues

$det(A-tI_n)=det(B-tI_n)|B=P^{-1}AP$

+ definition

  two matrices $A$ and $B$ are called similar if there exists an invertible matrix $P$ SUCH THAT $B=P^{-1}AP$

+ properties

  similar matrices have the same characteristic polynomial  and hence hence the same eigenvalues and multiplicities. In addition, their eigenspaces corresponding to the same eigenvalue have the same dimension.

## Diagonalization

+ definition

   An $n\times n$ matrix$ A$ is called diagonalizable if $ğ´ = ğ‘ƒğ·ğ‘ƒ^{-1}$1

  + $D$: $n\times n$ diagonal matrix

  + $P$: $ n\times n$ invertible matrix
    $$
    P= \left[
    \begin{matrix}
      p_1 & \cdots &p_n \\
    \end{matrix}
    \right]
    $$

    $$
    D = \left[
    \begin{matrix}
      d_1 & \cdots & 0 \\
      \vdots &\ddots  &\vdots  \\
      0 &\cdots &d_n
    \end{matrix}
    \right]
    $$

$$
ğ´ = ğ‘ƒğ·ğ‘ƒ^{-1}->Ap_i=d_ip_i \\

p_i \text{ is an eigenvector of A corresponding to eigenvalue }d_i
$$

+ properties

  There are n eigenvectors that form an invertible matrix

  There are n independent eigenvectors

  The eigenvectors of A can form a basis for $R^n$ .

+ how to diagonalize a matrix A?
  + Find n eigenvectors corresponding if possible, and form an invertible P
  + The eigenvalues corresponding to the eigenvectors in P form the diagonal matrix D
  
  <img src="/images/linear_algebra/diagonalization.png" alt="diagonalization" style="zoom:50%;" />

<img src="/images/linear_algebra/diagonalizable2.png" alt="diagonalization" style="zoom:50%;" />

+ When is a matrix diagonalizable?

  An n x n matrix A is diagonalizable if and only if both the following conditions are met

  + The characteristic polynomial of $A$ factors into a product of linear factors.
    $$
    det(A-tI_n)=(t-\lambda_1)^{m_1}(t-\lambda_2)^{m_2}\cdots(t-\lambda_k)^{m_k}(\cdots)
    $$
  
+ For each eigenvalue $\lambda$ of $A$, the multiplicity of $\lambda$ equals the dimension of the corresponding eigenspace.
  

<img src="/images/linear_algebra/diagonalizable3.png" alt="diagonalizable3" style="zoom:50%;" />

# Orthogonality

## basic concept

+ Norm

   Norm of vector v is the length of v

$$
\Vert v\Vert=\sqrt(v_1^2+v_2^2+\cdots+v_n^2)
$$

+ Distance

  The distance between two vectors u and v is defined by $\Vert v-u \Vert$

+ Dot product

   dot product of u and v is
  $$
  u \cdot v=u_1v_1+u_2v_2+\cdots+u_nv_n \\
  =\left[
  \begin{matrix}
    u_1 & u_2&\cdots &u_n \\
  \end{matrix}
  \right]
  \left[
  \begin{matrix}
    {v_1}\\
    {v_2}\\
    {\vdots}\\
    {v_n}
  \end{matrix}
  \right]
  =u^Tv
  $$

## Orthogonal

+ definition

  u and v are orthogonal if $ğ‘¢ \cdot ğ‘£ = 0$

+ Pythagorean Theorem
  $$
  \Vert u+v\Vert^2=\Vert u\Vert^2+\Vert v\Vert^2
  $$

+ Triangle Inequality
  $$
  \Vert u+v\Vert^2 \le\Vert u\Vert^2+\Vert v\Vert^2
  $$
  



##  Orthogonal Basis

+ Orthogonal Set
  + A set of vectors is called an orthogonal set if every pair of distinct vectors in the set is orthogonal.
  + Any orthogonal set of <font color=red>nonzero</font> vectors is linearly independent.

+ Orthonormal Set
  
+ A set of vectors is called an orthonormal set if it is an orthogonal set, and the norm of all the vectors is 1
  
+ Orthogonal Basis

  + A basis that is an orthogonal (orthonormal) set is called an orthogonal (orthonormal) basis

  + Let $ğ‘† = ğ‘£_1, ğ‘£_2,\cdots,ğ‘£_ğ‘˜$ be an orthogonal basis for a subspace $W$, and let $u $be a vector in $W$.
    $$
    u=c_1v_1+c_2v_2+\cdots+c_kv_k \\
    c_1=\frac{u \cdot v_1}{\Vert v_1 \Vert^2} \\c_2=\frac{u \cdot v_2}{\Vert v_2 \Vert^2}\\
    c_n=\frac{u \cdot v_n}{\Vert v_n \Vert^2}
    $$

  + Let ğ‘¢1, ğ‘¢2, â‹¯ , ğ‘¢ğ‘˜ be a basis of a subspace V. How to transform ğ‘¢1, ğ‘¢2, â‹¯ , ğ‘¢ğ‘˜ into an orthogonal basis ğ‘£1, ğ‘£2, â‹¯ , ğ‘£ğ‘˜ ?

    <img src="/images/linear_algebra/rothogonal_basis.png" alt="rothogonal_basis" style="zoom:50%;" />


## Orthogonal complementï¼ˆæ­£äº¤è¡¥ç©ºé—´ï¼‰

+ orthogonal complement
  + The orthogonal complement of a nonempty subset $S$ of $R^n$ denoted by $S^{\perp}$ , is the set of all vector in $R^n$ that are orthogonal to every vector in S. that is 
  + The orthogonal complement of any nonempty subset of $R^n$ is a subspace of  $R^n$

$$
S^{\perp}=\{v \in R^n:v \cdot u=0 \text{ for every }u \text{ in } S\}
$$

## Orthogonal Decomposition Theorem

+ let $W$ be a subspace of $R^n$ .Then, for any vector $u$ in $R^n$ .there exist unique vector $w$ in $W$ and $z$ in $W^{\perp}$ 

  such that $u=w+z$ .In addition, if $\{v_1,v_2,\cdots,v_K\}$ is an orthonormal basis for $W$ 
  $$
  
  $$
  