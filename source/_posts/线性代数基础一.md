---
title: 线性代数基础一
author: 范耀文
top: false
cover: false
toc: true
mathjax: true
date: 2020-02-25 09:46:58
img: /medias/banner/linear_math.jpg
coverImg: 
password:
summary:
	总结了线性代数常用的知识点。
tags:
	- 线性代数
categories:
	- 线性代数
typora-root-url: ..
---

# basic conception

## Zero Matrix

matrix with all zero entries, denoted by $O$ (any size) or $O_{m \times n}$

For example, a 2-by-3 zero matrix can be denoted
$$
O_{2 \times 3} = \left[
\begin{matrix}
  0 & 0 & 0\\
  0 & 0 & 0 \\
\end{matrix}
\right]
$$

## Identity Matrix

 must be square,Sometimes $ I_n $  is simply written as $I$ (any size). 
$$
I_3 = \left[
\begin{matrix}
  1 & 0 & 0\\
  0 & 1 & 0 \\
  0 & 0 & 1
\end{matrix}
\right]
$$
## $(0,1)$ matrices

all the entities of A are zeros and ones called (0,1)matrices.
$$
I_3 = \left[
\begin{matrix}
  0&1 & 1 & 0\\
  1 & 0 & 0&1 \\
  1&0&0&0\\
  0 & 1 & 0&0
\end{matrix}
\right]
$$
 <font color=red >get symmetirc matrix:$A^TA$</font>   

在生活中，我们会经常用到(0,1)矩阵(涉及图)

## Standard Vectors

we can write any vector $\left[
\begin{matrix}
  {a}\\
  {b}\\
\end{matrix}
\right]$in $R^2$ as a linear combination of the two vectors $\left[
\begin{matrix}
  {1}\\
  {0}\\
\end{matrix}
\right]$and$\left[
\begin{matrix}
  {0}\\
  {1}\\
\end{matrix}
\right]$

as follows:
$$
\left[
\begin{matrix}
  {a}\\
  {b}\\
\end{matrix}
\right]=a\left[
\begin{matrix}
  {1}\\
  {0}\\
\end{matrix}
\right]+b\left[
\begin{matrix}
  {0}\\
  {1}\\
\end{matrix}
\right]
$$
the vectors $\left[
\begin{matrix}
  {1}\\
  {0}\\
\end{matrix}
\right]$ and$\left[
\begin{matrix}
  {0}\\
  {1}\\
\end{matrix}
\right]$ are called the standard vectors of $R^2$ .

In general, we define the standard vectors of $R^n$ by
$$
e_1 =\left[
\begin{matrix}
  {1}\\
  {0}\\
  {\vdots}\\
  {0}
\end{matrix}
\right]
\qquad e_2 =\left[
\begin{matrix}
  {0}\\
  {1}\\
  {\vdots}\\
  {0}
\end{matrix}
\right]\qquad \cdots \qquad e_n =\left[
\begin{matrix}
  {0}\\
  {0}\\
  {\vdots}\\
  {1}
\end{matrix}\right]
$$

## Linear Combination

+ Given a vector set $\{u_1,u_2,\cdots,u_k\}$
+ The linear combination of the vectors in the set: 
  +  $𝑣 = 𝑐1𝑢1 + 𝑐2𝑢2 + ⋯ + 𝑐𝑘𝑢𝑘$
  + $ 𝑐1, 𝑐2, ⋯ , 𝑐𝑘 $ are scalars (Coefficients of linear combination)
+ the set of coefficients that express one vector as a linear combination of the others need not be unique
+ 以线性方程组的思想理解线性组合

$$
\begin{equation}  
\left\{
  \begin{array}{c}
  a_{11}x_1+a_{12}x_{2}+...+a_{1n}x_{n} = b_{1} \\
  a_{21}x_1+a_{22}x_{2}+...+a_{2n}x_{n} = b_{2} \\
          \vdots\\
  a_{m1}x_1+a_{m2}x_{n}+...+a_{mn}x_{n} = b_{n}
  \end{array}
\right.
\end{equation}
$$

$$
A =\left[
\begin{matrix}
  \mathbf{a_1}&\mathbf{a_2}&\mathbf{\cdots},\mathbf{a_n}
\end{matrix}
\right]
$$

$$
x =\left[
\begin{matrix}
  {a_1}\\
  {a_2}\\
  {\vdots}\\
  {a_n}
\end{matrix}
\right]
\qquad coeffients
$$

$$
\color{blue}{A}\color{red}{x} \color{black}=\color{red}{x_1}\color{blue}{a_1}\color{black}+\color{red}{x_2}\color{blue}{a_2}\color{black}+\color{red}{x_3}\color{blue}{a_3}\color{black}+\cdots\color{black}+\color{red}{x_n}\color{blue}{a_n}
$$

$A{\color{red}{x}}=b$：b是向量集A的线性组合，x是系数集，若方程组有无数解，则x不唯一。

## Span

+ A vector set $S=\{u_1,u_2,{\cdots},u_k\}$ 
+ Span of 𝑆 is the vector set of all linear combinations of $𝑢_1$, $𝑢_2$, ⋯ , $𝑢_k$ 
  + Denoted by 𝑆𝑝𝑎𝑛 $\{𝑢_1,𝑢_2, ⋯ ,𝑢_𝑘 \}$ or S𝑝𝑎𝑛  S
  
  + 𝑆𝑝𝑎𝑛 𝑆 = $\{c_1u_1+c_2u_2+\cdots+c_ku_kfor\quad all |c_1,c_2,\cdots,c_k\}$ 
  
  + Vector set $𝑉$ = 𝑆𝑝𝑎𝑛 $S$
  
     -  𝑆 is a generating set for 𝑉” or “𝑆 generates V” 
  -  A vector set generated by another vector set is called <font color=red >Space </font>
     
  

<img src="/images/linear_algebra/span.png" alt="span" style="zoom:50%;" />

+ m independent vectors can span $R^m$ --->More than m vectors in $R^m $ must be dependent
+ m independent vectors can span $R^m$ --->More than m vectors in $R^m$  must be dependent

## Dependent and Independent

A set of n vectors $\{𝒂_1,𝒂_2,\cdots,𝒂_𝑛\}$ is linear dependent 

 +   If there exist scalars 𝑥1, 𝑥2, ⋯ , 𝑥𝑛, <font color=red>not all zero</font>,

 +  such that 

    $$𝑥_1𝒂_1 + 𝑥_2𝒂_2 +\cdots+ 𝑥_𝑛𝒂_𝑛=0$$

 +   A set of n vectors $\{𝒂_1, 𝒂_2,\cdots, 𝒂_𝑛\}$ is linear independent 

     $$𝑥_1𝒂_1 + 𝑥_2𝒂_2 +\cdots+ 𝑥_𝑛𝒂_𝑛=0$$

    Only if $𝑥1 = 𝑥2 = ⋯ = 𝑥𝑛 = 0$ 

<font color=blue >Any set contains zero vector would be linear dependent</font> 

## Reduced row echelon form

+ Row Echelon Form

  +  Each nonzero row lies above every zero row（all its entries are 0）
  + The leading entries are in echelon form(阶梯型)

+ reduced row echelon form

  + The matrix is in row echelon form

  + The columns containing the leading entries are standard vectors

    <img src="/images/linear_algebra/ref.png" alt="ref" style="zoom:50%;" />

+ RREF在求解线性方程组中的应用
  + 一个线性方程组可以用增广矩阵来表示，对增广矩阵进行任何的基本行操作，解集不变
  + 形如简化阶梯形矩阵可以很容器求出解集。
    - 当rref矩阵包含除最后一个元素非零的一行：无解
    - 当rref矩阵的系数矩阵为单位矩阵：唯一解
    - 其他情况：无穷解
      - basic variables
      - free variables

+ RREF的性质
  + COLUMNS: If 𝒂𝒋 is a linear combination of other columns of A( with the same coefficients)<---> 𝒓𝒋 is a linear combination of the corresponding columns of R with the same coefficients
  + ROWS: span$\{row_1,row_2,\cdots,row_n\}$ <--->span$\{row^1,row^2,\cdots,row^n\}$ 
  + The pivot columns are linear independent
  + The non-pivot columns are the linear combination of the previous pivot columns.

## Having Solution or Not of system of linear equations

+ Is 𝑏 a linear combination of columns of 𝐴?
+ Is 𝑏 in the span of the columns of 𝐴?
+ is consistent?
+ is b in Col A?

## How many solutions

+ Columns of A are dependent <--->If Ax=b have solution, it will have Infinite solutions

+ Columns of A are independent <--->if Ax=b have solution ,it will have only one solution


## Rank and Nullity 

+ definition_1
  + The rank of a matrix is defined as the maximum number of <font color=blue>linearly independent columns</font>> in the matrix. 
  + Nullity = <font color=red>Number of columns</font> - rank
+ definition_2
  + Rank=Number of Pivot Column
+ definition_3
  + Rank=Number of Non-zero rows
+ definition_4
  + Rank=Number of Basic Variables
+ definition_5
  + Rank= Dim (Col A): dimension of column space
  + Dimension of the range of A
+ definition_6
  + Rank=Dim(Row A): dimension of row space
+ properties
  + Rank $A$=n Nullity=0  <--->   Columns of $A$ are independent
  + Given a $m\times n$ matrix $A$:$
    + Matrix $A$ is full rank if Rank $A$ = min(m,n)
    + In $R^m$, you cannot find more than m vectors that are independent
    + rank $A$ = rank $A^T$  

## Inverse of a Matrix

+ definition:

  $A$ is called invertible if there is a matrix B such that $𝐴𝐵 = 𝐼$  and $𝐵𝐴 = I$ 

+ elementary matrices

  Every elementary row operation can be performed by matrix multiplication

  +  Interchange
    $$
    \color{blue}{\left[
    \begin{matrix}
      0 & 1 \\
      1 & 0 \\
    \end{matrix}
    \right]}
    \color{black}{\left[
    \begin{matrix}
      a & b \\
      c & d \\
    \end{matrix}
    \right]
    =
    \left[
    \begin{matrix}
      c & d \\
      a & b \\
    \end{matrix}
    \right]}
    $$

  + Scaling
    $$
    \color{blue}{\left[
    \begin{matrix}
      1 & 0 \\
      0 & k \\
    \end{matrix}
    \right]}
    \color{black}{\left[
    \begin{matrix}
      a & b \\
      c & d \\
    \end{matrix}
    \right]
    =
    \left[
    \begin{matrix}
      a & b \\
      kc & kd \\
    \end{matrix}
    \right]}
    $$

  +  Adding k times row i to row j
    $$
    \color{blue}{\left[
    \begin{matrix}
      1 & 0 \\
      k & 1 \\
    \end{matrix}
    \right]}
    \color{black}{\left[
    \begin{matrix}
      a & b \\
      c & d \\
    \end{matrix}
    \right]
    =
    \left[
    \begin{matrix}
      a & b \\
      ka+c & kb+d \\
    \end{matrix}
    \right]}
    $$

+ Inverse of Elementary Matrix

  Reverse elementary row operation

$$
E_1=\left[
\begin{matrix}
  1 & 0 \\
  k & 1 \\
\end{matrix}
\right]
E_1^{-1}=\left[
\begin{matrix}
  1 & 0 \\
  -k & 1 \\
\end{matrix}
\right]
$$

+ RREF & Elementary Matrix

  + Let A be an $m \times n$ matrix with reduced row echelon form R

  $$
  𝐸_{𝑘}\cdots 𝐸_{2}𝐸_1𝐴 = R
  $$

  + There exists an invertible m x m matrix P such that PA=R
    $$
    𝑃^{−1} = 𝐸_1^{−1}𝐸_2^{−1}\cdots 𝐸_𝑘^{−1}
    $$

+ 判断一个矩阵是否可逆
  
  Let A be an n x n matrix. A is invertible if and only if
  
  + The columns of $A$ span $R_n$
  + For every $b$  in $R_n$ , the system $Ax=b$  is consistent
  + The rank of $A$  is $n$ 
  + The columns of $A$  are linear independent
  + The only solution to $Ax=0$  is the zero vector
  + The nullity of $A$  is zero
  + The reduced row echelon form of $A$ is $I_n$
  + $A$ is a product of elementary matrices
  + There exists an $n \times n$ matrix $B$ such that $BA = I_n$
  + There exists an $n \times n $ matrix $C$ such that $AC = I_n$ 

# Subspace

+ definition
  +  The zero vector $0$  belongs to $V$ 
  +  If u and $w$  belong to $V$ , then $u+w$  belongs to $V$ (Closed under (vector) addition)
  +  If u belongs to $V$ , and $c$  is a scalar, then $cu$  belongs to $V$ (Closed under scalar multiplication)

+ special

  zero subspace: the set $w$ consisting of only the zero vector in $R_n$ is a subspace of $R_n$ called the **zero subspace** . A subspace of $R_n$ other than ${0}$ is called a **nonzero subspace**.

+ properties
  + The span of a vector set is a subspace
  + The **null space** of a matrix A is the solution set of Ax=0. It is denoted as Null A.
  + The **column space** of a matrix A is the span of its columns. it is denoted by Col A.
  + **Row space** of a matrix A is the span of its rows. It is denoted as Row A. 

## Basis

+ definition

  + Let $V$ be a nonzero subspace of $R_n$. A basis $B$ for $V$ is a linearly independent generation set of $V$ 
  + $\{e_1,e_2,\cdots,e_n\}$ is a basis for $R_n$ 
    +  $\{e_1,e_2,\cdots,e_n \}$ is independent
    +  $\{e_1 , e_2 , \cdots, e_n \}$ generates $R$

  + The pivot columns of a matrix form a basis for its columns space.

+ properties

  + $S$ is contained in Span $S$
  + If a finite set S’ is contained in Span $S$, then Span $S’$ is also contained in Span $S$
  + For any vector $z$, Span $S$ = Span $S∪{z}$ if and only if z belongs to the Span $ S$

+ Theorem 

  +  A basis is the <font color=blue>smallest generation set</font>.

  + A basis is the <font color=red>largest independent vector set</font> in the subspace.

  +  Any two bases for a subspace <font color=green>contain the same number of vectors</font>.

    + The number of vectors in a basis for a nonzero subspace V is called <font color=red>dimension</font> of $V $(<font color=red>dim</font> $V$).

+ Subspaces associated with a Matrix

  <img src="/images/linear_algebra/basis.png" alt="basis" style="zoom:50%;" />

## linear transformations 

+ **function**

  let $S_1$ and $S_2$ be subsets of $R_n$ and $R_m$, respectively. A **function** $f$ from $S_1$ and $S_2$. written f :$S_1->S_2$. is a rule that assigns to each vector $v$ in $S_1$ A unique vector $f(v)$ in $S_2$ .The vector $f(v)$ is called the image of $v$ (under $f$). The set $S_1$ is called the **domain** of a function $f$. and the set $S_2$ is called the **codomain** of $f$. The $range$ of $f$ is defined to be the set of images $f(v)$ for all $v$ in $S_1$.
  
  <img src="/images/linear_algebra/T_a.jpg" alt="function" style="zoom:50%;" />

+ **matrix transformation**

  <font color=blue>let A be an $m \times n$ matrix. The function $T_A:R^n ->R^m$ defined by $T_A(X)=Ax$ for all $x$ in $R^n$ is called the **matrix transformation** induced by A</font>.

  + rotations 

  $$
  A_0 = \left[
  \begin{matrix}
    cos \theta & -sin \theta\\
    sin \theta & cos \theta 
  \end{matrix}
  \right]
  $$
  + Expansion
    $$
    T_A = \left[
    \begin{matrix}
      2 & 0 \\
      0 & 2  \\
    \end{matrix}
    \right]
    $$

  + compression
    $$
    T_A = \left[
    \begin{matrix}
      0.5 & 0 \\
      0 & 0.5  \\
    \end{matrix}
    \right]
    $$

  + projections

    将一个向量映射到$xy-plane$

  $$
  T_A = \left[
  \begin{matrix}
    1 & 0 & 0\\
    0 & 1 & 0 \\
    0 & 0 & 0
  \end{matrix}
  \right]
  $$

  + shear

$$
T_A = \left[
\begin{matrix}
  1 & k \\
  0 & 1  \\
\end{matrix}
\right]
$$

​		  



+ **linear transformation**

  <font color=RED>A function T from $R^n$ to $R^m$. written $T: R^n ->R^m$. is called linear transformation if for all vectors $u$ and $v$ in $R^n$ and all scalar c </FONT>.

  + $T(u+v) = T(u)+T(v)$
  + $T(cu) = cT(u)$

  special linear transformation

  + matrix transformation
  + identity transformation
  + zero transformation

+ standard matrix

  let $T: R^n ->R^m$ be a linear transformation. we call the $m \times n$ matrix 

  $A=[T(e_1) T(e_2)\cdots T(e_n)]$ 

  $T(v)=Av$ 

  

## coordinate system

+ definition

  + Let vector set $B=\{𝑢_1,𝑢_2, ⋯,𝑢_𝑛$ be a **<font color=red>basis</font>** for a subspace $R^n$ ,$B$ is a coordinate system

  + For any $v$ in $R^n$ , there are unique scalars $𝑐_1,𝑐_2,⋯,𝑐_𝑛$ such that $𝑣 = 𝑐_1𝑢_1 + 𝑐_2𝑢_2 + ⋯ + 𝑐_𝑛𝑢_n$ , <font color=red>**$B$ -coordinate vector**</font> of $ v$:
    $$
    [v]_B =\left[
    \begin{matrix}
      {c_1}\\
      {c_2}\\
      {\vdots}\\
      {c_n}
    \end{matrix}
    \right]
    \in R^n
    $$

<img src="/images/linear_algebra/coordinate.png" alt="coordinate system" style="zoom:50%;" />

## Linear Function in Coordinate System

<img src="/images/linear_algebra/linear_opertor.png" alt="linear_opertor" style="zoom:50%;" />

<img src="/images/linear_algebra/matirx_representation.png" alt="matirx_representation" style="zoom:50%;" />

# EIGENVALUES, EIGENVECTORS

## Eigenvalues and Eigenvectors

+ definition

  If $𝐴𝑣 = 𝜆𝑣$ ($𝑣$ is a vector, $𝜆$ is a scalar)

  + $𝑣$ is an eigenvector of $A$(excluding zero vector)
  + $𝜆$ is an eigenvalue of $A$ that corresponds to $v$

+ properties
  + An eigenvector of A corresponds to a unique eigenvalue
  + An eigenvalue of A has infinitely many eigenvectors

+ eigenspace

  $𝑁𝑢𝑙𝑙(A − \lambda I_n )$

+ how to look for the eigenvalue

  $det(A-tI_n)=0$ characteristic equation

  $det(A-tI_n)$ characteristic polynomial

  **Eigenvalues are the roots of characteristic polynomial or solutions of characteristic equation**

## Characteristic Polynomial

+ definition

  Characteristic polynomial of A is

$$
det(A-tI_n)=(t-\lambda_1)^{m_1}(t-\lambda_2)^{m_2}\cdots(t-\lambda_k)^{m_k}(\cdots)
$$

​	$m_k \text{ is call the multiplicity of }\lambda$  

An n x n matrix A have less than or equal to n eigenvalues

+ properties

  let $\lambda$ be an eigenvalue of a matrix A .the dimension of the eigenspace of $A$ corresponding to $\lambda$  is less than or equal to the multiplicity of $\lambda$.

## The eigenvalues of similar matrices

Similar matrices have the same characteristic polynomials

The same Eigenvalues

$det(A-tI_n)=det(B-tI_n)|B=P^{-1}AP$

+ definition

  two matrices $A$ and $B$ are called similar if there exists an invertible matrix $P$ SUCH THAT $B=P^{-1}AP$

+ properties

  similar matrices have the same characteristic polynomial  and hence hence the same eigenvalues and multiplicities. In addition, their eigenspaces corresponding to the same eigenvalue have the same dimension.

## Diagonalization

+ definition

   An $n\times n$ matrix$ A$ is called diagonalizable if $𝐴 = 𝑃𝐷𝑃^{-1}$1

  + $D$: $n\times n$ diagonal matrix

  + $P$: $ n\times n$ invertible matrix
    $$
    P= \left[
    \begin{matrix}
      p_1 & \cdots &p_n \\
    \end{matrix}
    \right]
    $$

    $$
    D = \left[
    \begin{matrix}
      d_1 & \cdots & 0 \\
      \vdots &\ddots  &\vdots  \\
      0 &\cdots &d_n
    \end{matrix}
    \right]
    $$

$$
𝐴 = 𝑃𝐷𝑃^{-1}->Ap_i=d_ip_i \\

p_i \text{ is an eigenvector of A corresponding to eigenvalue }d_i
$$

+ properties

  There are n eigenvectors that form an invertible matrix

  There are n independent eigenvectors

  The eigenvectors of A can form a basis for $R^n$ .

+ how to diagonalize a matrix A?
  + Find n eigenvectors corresponding if possible, and form an invertible P
  + The eigenvalues corresponding to the eigenvectors in P form the diagonal matrix D
  
  <img src="/images/linear_algebra/diagonalization.png" alt="diagonalization" style="zoom:50%;" />

<img src="/images/linear_algebra/diagonalizable2.png" alt="diagonalization" style="zoom:50%;" />

+ When is a matrix diagonalizable?

  An n x n matrix A is diagonalizable if and only if both the following conditions are met

  + The characteristic polynomial of $A$ factors into a product of linear factors.
    $$
    det(A-tI_n)=(t-\lambda_1)^{m_1}(t-\lambda_2)^{m_2}\cdots(t-\lambda_k)^{m_k}(\cdots)
    $$
  
+ For each eigenvalue $\lambda$ of $A$, the multiplicity of $\lambda$ equals the dimension of the corresponding eigenspace.
  

<img src="/images/linear_algebra/diagonalizable3.png" alt="diagonalizable3" style="zoom:50%;" />

# Orthogonality

## basic concept

+ Norm

   Norm of vector v is the length of v

$$
\Vert v\Vert=\sqrt(v_1^2+v_2^2+\cdots+v_n^2)
$$

+ Distance

  The distance between two vectors u and v is defined by $\Vert v-u \Vert$

+ Dot product

   dot product of u and v is
  $$
  u \cdot v=u_1v_1+u_2v_2+\cdots+u_nv_n \\
  =\left[
  \begin{matrix}
    u_1 & u_2&\cdots &u_n \\
  \end{matrix}
  \right]
  \left[
  \begin{matrix}
    {v_1}\\
    {v_2}\\
    {\vdots}\\
    {v_n}
  \end{matrix}
  \right]
  =u^Tv
  $$

## Orthogonal

+ definition

  u and v are orthogonal if $𝑢 \cdot 𝑣 = 0$

+ Pythagorean Theorem
  $$
  \Vert u+v\Vert^2=\Vert u\Vert^2+\Vert v\Vert^2
  $$

+ Triangle Inequality
  $$
  \Vert u+v\Vert^2 \le\Vert u\Vert^2+\Vert v\Vert^2
  $$
  



##  Orthogonal Basis

+ Orthogonal Set
  + A set of vectors is called an orthogonal set if every pair of distinct vectors in the set is orthogonal.
  + Any orthogonal set of <font color=red>nonzero</font> vectors is linearly independent.

+ Orthonormal Set
  
+ A set of vectors is called an orthonormal set if it is an orthogonal set, and the norm of all the vectors is 1
  
+ Orthogonal Basis

  + A basis that is an orthogonal (orthonormal) set is called an orthogonal (orthonormal) basis

  + Let $𝑆 = 𝑣_1, 𝑣_2,\cdots,𝑣_𝑘$ be an orthogonal basis for a subspace $W$, and let $u $be a vector in $W$.
    $$
    u=c_1v_1+c_2v_2+\cdots+c_kv_k \\
    c_1=\frac{u \cdot v_1}{\Vert v_1 \Vert^2} \\c_2=\frac{u \cdot v_2}{\Vert v_2 \Vert^2}\\
    c_n=\frac{u \cdot v_n}{\Vert v_n \Vert^2}
    $$

  + Let 𝑢1, 𝑢2, ⋯ , 𝑢𝑘 be a basis of a subspace V. How to transform 𝑢1, 𝑢2, ⋯ , 𝑢𝑘 into an orthogonal basis 𝑣1, 𝑣2, ⋯ , 𝑣𝑘 ?

    <img src="/images/linear_algebra/rothogonal_basis.png" alt="rothogonal_basis" style="zoom:50%;" />


## Orthogonal complement（正交补空间）

+ orthogonal complement
  + The orthogonal complement of a nonempty subset $S$ of $R^n$ denoted by $S^{\perp}$ , is the set of all vector in $R^n$ that are orthogonal to every vector in S. that is 
  + The orthogonal complement of any nonempty subset of $R^n$ is a subspace of  $R^n$

$$
S^{\perp}=\{v \in R^n:v \cdot u=0 \text{ for every }u \text{ in } S\}
$$

## Orthogonal Decomposition Theorem

+ let $W$ be a subspace of $R^n$ .Then, for any vector $u$ in $R^n$ .there exist unique vector $w$ in $W$ and $z$ in $W^{\perp}$ 

  such that $u=w+z$ .In addition, if $\{v_1,v_2,\cdots,v_K\}$ is an orthonormal basis for $W$ 
  $$
  
  $$
  